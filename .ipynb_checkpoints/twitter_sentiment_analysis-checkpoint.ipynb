{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries which will be used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # for the regular expressions\n",
    "import nltk  # for text manipulation\n",
    "import string  \n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_tweets_anuFYb8.csv\")\n",
    "train = pd.read_csv(\"train_E6oV3lV.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion phase of the cleaning process of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['label'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['label'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a glimpse of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train = train['tweet'].str.len()\n",
    "length_test = test['tweet'].str.len()\n",
    "plt.hist(length_train, bins=20, label=\"train_tweets\")\n",
    "plt.hist(length_test, bins=20, label=\"test_tweets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to clean, but first, let's combine both train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = train.append(test, ignore_index=True)\n",
    "combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['tidy_tweet'] = np.vectorize(remove_pattern)(combine['tweet'], \"@[\\w]*\")\n",
    "combine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation, Number, and Special Character remover\n",
    "\n",
    "Here we replace everthin except characters and hashtags with spaces. The regular expression \"[^a-zA-Z#]\" means anything except alphabeths and '#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['tidy_tweet'] = combine['tidy_tweet'].str.replace(\"^a-zA-Z#\", \" \")\n",
    "combine.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['tidy_tweet'] = combine['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet = combine['tidy_tweet'].apply(lambda x: x.split()) # tokenizing\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can normalize the tokenized tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "tokenize_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's stitch these tokens back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "combine['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in combine['tidy_tweet']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, \n",
    "max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(10, 7)) \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words = ' '.join([text for text in combine['tidy_tweet'][combine['label'] == 0]])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21,\n",
    "                     max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = ' '.join([text for text in combine['tidy_tweet'][combine['label'] == 1]])\n",
    "wordcloud = WordCloud(width=800, height=500, \n",
    "random_state=21, max_font_size=110).generate(negative_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect hashtags\n",
    "def hashtag_extract(x):\n",
    "    hashtags = []\n",
    "    #Loop over the words in tweet\n",
    "    for i in x:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "        \n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hashtags from non racist/sexist tweets\n",
    "\n",
    "HT_regular = hashtag_extract(combine['tidy_tweet'][combine['label'] == 0])\n",
    "\n",
    "# Extract hashtags from racist/sexist tweets\n",
    "HT_negative = hashtag_extract(combine['tidy_tweet'][combine['label'] == 1])\n",
    "\n",
    "# unresting list\n",
    "HT_regular = sum(HT_regular,[])\n",
    "HT_negative = sum(HT_negative,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nltk.FreqDist(HT_regular)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                 'Count': list(a.values())})\n",
    "#Selectint top 10 most frequent hashtags\n",
    "d = d.nlargest(columns=\"Count\", n=10)\n",
    "plt.figure(figsize=(16, 5))\n",
    "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = nltk.FreqDist(HT_negative)\n",
    "e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n",
    "# selecting top 10 most frequent hashtags\n",
    "e = e.nlargest(columns='Count', n=10)\n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=e, x= \"Hashtag\", y= \"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, \n",
    "max_features=1000, stop_words='english')\n",
    "# bag of words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(combine['tidy_tweet'])\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from smart_open import smart_open\n",
    "import gensim\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, \n",
    "max_features=1000, stop_words='english')\n",
    "\n",
    "#TF IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(combine['tidy_tweet'])\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "tokenized_tweet = combine['tidy_tweet'].apply(lambda x: x.split()) #tokenizing\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=200, #desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg =1, # 1 for skip-gram model\n",
    "            negative = 10, # for negative sampling\n",
    "            workers = 2, #no.of cores\n",
    "            seed = 34)\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples=len(combine['tidy_tweet']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.wv.most_similar(positive='dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.wv.most_similar(positive='trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v['food'] # The length of the vector is 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is nor=t in vocabulary\n",
    "                continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing word2vec feature set\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays) \n",
    "\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's labelise and or tag each tkenised tweer with unique IDs, by using Gensim's LabeledSentence() function to implement doc2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n",
    "    return output\n",
    "\n",
    "labeled_tweets = add_label(tokenized_tweet) # label all the tweets\n",
    "\n",
    "labeled_tweets[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v = gensim.models.Doc2Vec(dm=1, #dm = 1 'distribution memory' model \n",
    "                                dm_mean=21, # dm = 1 fr using mean of the context word vectors\n",
    "                                size=200, # no. of desired features\n",
    "                                window=5, # width of the contwext window\n",
    "                                negative=7, # if > 0 the negative sampling will be used\n",
    "                                min_count=5, # Ignores all ords with total frequency lower than 2.\n",
    "                                workers=3, # no. of cores\n",
    "                                alpha=0.1, # learning rate\n",
    "                                seed=23)\n",
    "\n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n",
    "\n",
    "model_d2v.train(labeled_tweets, total_examples=len(combine['tidy_tweet']), epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepparing doc2vec Feature Set\n",
    "docvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
    "for i in range(len(combine)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1, 200))\n",
    "    \n",
    "docvec_df = pd.DataFrame(docvec_arrays)\n",
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model as building models on the datasets with different features such as bags-of-words, word2vec, tf-idf & doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Logistic Regression to train our model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    " \n",
    "#bag-of-word features\n",
    "\n",
    "train_bow = bow[:31962,:]\n",
    "test_bow = bow[31962:,:]\n",
    "\n",
    "# splitting data into training and validation set\n",
    "x_train_bow, x_valid_bow, y_train, y_valid = train_test_split(train_bow, \n",
    "train['label'], random_state=42, test_size=0.3)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(x_train_bow, y_train) # training the model\n",
    "\n",
    "prediction = lreg.predict_proba(x_valid_bow) # prediction on the validation set\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "\n",
    "f1_score(y_valid, prediction_int) # calculating f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction on test dataset and make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lreg.predict_proba(test_bow)\n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "test['label'] = test_pred_int\n",
    "submission = test[['id', 'label']]\n",
    "submission.to_csv('sub_lreg_bow.csv', index=False)\n",
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf features\n",
    "train_tfidf = tfidf[:31962,:]\n",
    "test_tfidf = tfidf[31962:,:]\n",
    "x_train_tfidf = train_tfidf[y_train.index] \n",
    "x_valid_tfidf = train_tfidf[y_valid.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg.fit(x_train_tfidf, y_train)\n",
    "prediction_tfidf = lreg.predict_proba(x_valid_tfidf)\n",
    "prediction_int_tfidf = prediction_tfidf[:,1] >= 0.3\n",
    "prediction_int_tfidf = prediction_int_tfidf.astype(np.int)\n",
    "f1_score(y_valid, prediction_int_tfidf) # calculating f1 score for the validtion set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Features\n",
    "train_w2v = wordvec_df.iloc[:31962,:]\n",
    "test_w2v = wordvec_df.iloc[31962:,:]\n",
    "x_train_w2v = train_w2v.iloc[y_train.index,:]\n",
    "x_valid_w2v = train_w2v.iloc[y_valid.index,:]\n",
    "\n",
    "\n",
    "lreg.fit(x_train_w2v, y_train)\n",
    "prediction_w2v = lreg.predict_proba(x_valid_w2v)\n",
    "prediction_int_w2v = prediction_w2v[:,1] >= 0.3\n",
    "prediction_int_w2v = prediction_int_w2v.astype(np.int)\n",
    "f1_score(y_valid, prediction_int_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec features\n",
    "train_d2v = docvec_df.iloc[:31962,:]\n",
    "test_d2v = docvec_df.iloc[31962:,:]\n",
    "x_train_d2v = train_d2v.iloc[y_train.index,:]\n",
    "x_valid_d2v = train_d2v.iloc[y_valid.index,:]\n",
    "\n",
    "lreg.fit(x_train_d2v, y_train)\n",
    "prediction_d2v = lreg.predict_proba(x_valid_d2v)\n",
    "prediction_int_d2v = prediction_d2v[:,1] >= 0.3\n",
    "prediction_int_d2v = prediction_int_d2v.astype(np.int)\n",
    "f1_score(y_valid, prediction_int_d2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#bag-of-words features\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(x_train_bow, y_train)\n",
    "prediction_svm = svc.predict_proba(x_valid_bow)\n",
    "prediction_int_svm = prediction_svm[:,1] >= 0.3\n",
    "prediction_int_svm = prediction_int_svm.astype(np.int)\n",
    "f1_score(y_valid, prediction_int_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_svm = svc.predict_proba(test_bow)\n",
    "test_pred_int_bow = test_pred_svm[:,1] >= 0.3\n",
    "test_pred_int_bow = test_pred_int_bow.astype(np.int)\n",
    "test['label'] = test_pred_int_bow\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_svm_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf features\n",
    "\n",
    "svc_tfidf = svm.SVC(kernel='linear', C=1, probability=True).fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_svm_tfidf = svc.predict_proba(x_train_tfidf)\n",
    "prediction_int_svm_tfidf = prediction[:,1] >= 0.3\n",
    "prediction_int_svm_tfidf = prediction_int_svm_tfidf.astype(np.int)\n",
    "f1_score(y_valid, prediction_int_svm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec features\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(x_train_w2v, y_train)\n",
    "prediction_svm_w2v = svc.predict_proba(x_valid_w2v)\n",
    "prediction_svm_w2v_int = prediction_svm_w2v[:,1] >= 0.3\n",
    "prediction_svm_w2v_int = prediction_svm_w2v_int.astype(np.int)\n",
    "f1_score(y_valid, prediction_svm_w2v_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(x_train_d2v, y_train)\n",
    "prediction_svm_d2v = svc.predict_proba(x_valid_d2v)\n",
    "prediction_svm_d2v_int = prediction_svm_d2v[:,1] >= 0.3\n",
    "prediction_svm_d2v_int = prediction_svm_d2v_int.astype(np.int)\n",
    "# validation score\n",
    "f1_score(y_valid, prediction_svm_d2v_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The use RandomForest for the data fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# bag-of-words features\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(x_train_bow, y_train)\n",
    "prediction_rf = rf.predict(x_valid_bow)\n",
    "# validation score\n",
    "f1_score(y_valid, prediction_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "test_pred_bow = rf.predict(test_bow) \n",
    "test['label'] = test_pred\n",
    "submission = test[['id', 'label']]\n",
    "submission.to_csv('sub_rf_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "rf = RandomForestClassifier(n_estimators =400, random_state=11).fit(x_train_tfidf, y_train)\n",
    "prediction_rf_tfidf = rf.predict(x_valid_tfidf)\n",
    "f1_score(y_valid, prediction_rf_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec features\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(x_train_w2v, y_train)\n",
    "prediction_w2v = rf.predict(x_valid_w2v)\n",
    "f1_score(y_valid, prediction_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec features\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(x_train_d2v, y_train)\n",
    "prediction_d2v = rf.predict(x_valid_d2v)\n",
    "f1_score(y_valid, prediction_d2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Gradient Boosting (xgBoost) model fitting with available features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Bag-of-Words Features\n",
    "xgb_model = XGBClassifier(maxdepth=6, n_estimators=1000).fit(x_train_bow, y_train)\n",
    "prediction_xgb = xgb_model.predict(x_valid_bow)\n",
    "f1_score(y_valid, prediction_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf features\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(x_train_tfidf, y_train)\n",
    "prediction_xgb_tfidf = xgb.predict(x_valid_tfidf)\n",
    "f1_score(y_valid, prediction_xgb_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec Features\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread=3).fit(x_train_w2v, y_train)\n",
    "prediction_xgb_w2v = xgb.predict(x_valid_w2v)\n",
    "f1_score(y_valid, prediction_xgb_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec Features\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(x_train_d2v, y_train)\n",
    "prediction_xgb_d2v = xgb.predict(x_valid_d2v)\n",
    "f1_score(y_valid, prediction_xgb_d2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tuning the xgboost parameter especially w2v as the model that gave us highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DMatrix as parameter with both features and target\n",
    "d_train = xgb.DMatrix(x_train_w2v, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid_w2v, label=y_valid)\n",
    "d_test = xgb.DMatrix(test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters that are going to tune\n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_deoth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample':1,\n",
    "    'colsample_bytree': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the customer evaluation to calculate f1_score\n",
    "def custom_eval(preds, d_train):\n",
    "    labels = d_train.get_label().astype(np.int)\n",
    "    preds = (preds >= 0.3).astype(np.int)\n",
    "    return [('f1_score', f1_score(labels, preds))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General approach for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning max_depth and min_child_weight\n",
    "griedsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(6, 10)\n",
    "        for min_child_weight in range(5, 8)\n",
    "    \n",
    "]\n",
    "\n",
    "max_f1 = 0. # initializing with 0\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in griedsearch_params:\n",
    "    print(\"CV with max_depth={}, min__child_weight={}\".format(\n",
    "                    max_depth,\n",
    "                    min_child_weight))\n",
    "    # update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_results = xgb.cv(dtrain=d_train, \n",
    "                        params=params, \n",
    "                        nfold=3,\n",
    "                    num_boost_round=50,\n",
    "                        early_stopping_rounds=10,\n",
    "                        feval=custom_eval,\n",
    "                        #metrics=\"rmse\", \n",
    "                        maximize=True, \n",
    "                        seed=123\n",
    "                       )\n",
    "    \n",
    "#Finding the best F1 Score\n",
    "\n",
    "mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "\n",
    "\n",
    "boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "print(\"\\tF1 Score {} for {} rounds.\".format(mean_f1, boost_rounds))\n",
    "if mean_f1 > max_f1:\n",
    "    max_f1 = mean_f1\n",
    "    best_params = (max_depth, min_child_weight)\n",
    "    \n",
    "    \n",
    "print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))\n",
    "\n",
    "# CV with max_depth=6, min_child_weight=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating the max_depth and min_child_weight parameters\n",
    "params['max_depth'] = 8\n",
    "param['min_child_weight'] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning subsample and colsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(5, 10)]\n",
    "    for colsample in [i/10. for i in range(5,10)]\n",
    "]\n",
    "\n",
    "max_f1 = 0.\n",
    "best_params = None\n",
    "for subsample, colsample in gridsearch_params:\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                                subsample,\n",
    "                                colsample))\n",
    "    \n",
    "    # Update our parameters\n",
    "    params['colsample'] = colsample\n",
    "    params['subsample'] = subsample\n",
    "    cv_results = xgb.cv(\n",
    "            params=params,\n",
    "            dtrain=d_train,\n",
    "            feval = custom_eval,\n",
    "            num_boost_round=200,\n",
    "            maximize=True,\n",
    "            seed=16,\n",
    "            nfold=5,\n",
    "            early_stopping_rounds=10)\n",
    "    \n",
    "    # Ffinding best F1 Score\n",
    "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "    if mean_f1 > max_f1:\n",
    "        max_f1 = mean_f1\n",
    "        best_params = (subsample, colsample)\n",
    "        \n",
    "print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[1], max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the susample and coldsample_bytree\n",
    "\n",
    "params['subsample'] = .9\n",
    "params['colsample_bytree'] = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's tune the learning rate\n",
    "\n",
    "max_f1 = 0.\n",
    "best_params = None\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "    \n",
    "    #Update ETA\n",
    "    params['eta'] = eta\n",
    "    \n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "            params=params,\n",
    "            dtrain=d_train,\n",
    "            feval=custom_eval,\n",
    "            num_boost_round=1000,\n",
    "            maximize=True,\n",
    "            seed=16,\n",
    "            nfold=5,\n",
    "            early_stopping_rounds=20\n",
    "    )\n",
    "    \n",
    "    # Finding best f1 score\n",
    "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "    if mean_f1 > max_f1:\n",
    "        mean_f1 = mean_f1\n",
    "        best_params = eta\n",
    "\n",
    "print(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List look at the final list of tuned parameters\n",
    "\n",
    "params\n",
    "\n",
    "{\n",
    "    'colsample': 0.9,\n",
    "    'colsample_bytree': 0.5, 'eta': 0.1,\n",
    "    'max_depth': 8, 'min_child_weight': 6,\n",
    "    'objective': 'binary:logistic',\n",
    "    'subsample': 0.9\n",
    "}\n",
    "\n",
    "xgb_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    feval=custom_eval,\n",
    "    num_boost_round=1000,\n",
    "    maximize=True,\n",
    "    evals=[(d_valid, 'Validation')],\n",
    "    early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
